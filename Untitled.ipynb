{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34818f33-3b65-4ce1-bb44-15bf20d8f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.96 GB, other allocations: 146.67 MB, max allowed: 18.13 GB). Tried to allocate 96.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 111\u001b[0m\n\u001b[1;32m    103\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    104\u001b[0m     model\u001b[38;5;241m=\u001b[39mbert_model,\n\u001b[1;32m    105\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m    106\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    107\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Train and Save Model\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m bert_tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1950\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1950\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1962\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1964\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:540\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.96 GB, other allocations: 146.67 MB, max allowed: 18.13 GB). Tried to allocate 96.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import gradio as gr\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize Models and Tokenizers\n",
    "def initialize_model_and_tokenizer(model_name):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load BERT and RoBERTa models and tokenizers\n",
    "bert_model, bert_tokenizer = initialize_model_and_tokenizer(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "roberta_model, roberta_tokenizer = initialize_model_and_tokenizer(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Device Selection\n",
    "def select_device():\n",
    "    return torch.device(\n",
    "        \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "device = select_device()\n",
    "bert_model.to(device)\n",
    "roberta_model.to(device)\n",
    "\n",
    "# Create Pipelines\n",
    "def create_pipeline(model, tokenizer):\n",
    "    return pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0 if device != \"cpu\" else -1)\n",
    "\n",
    "bert_pipeline = create_pipeline(bert_model, bert_tokenizer)\n",
    "roberta_pipeline = create_pipeline(roberta_model, roberta_tokenizer)\n",
    "\n",
    "# Question Answering Function\n",
    "def evaluate_question(context, question, model_choice):\n",
    "    pipelines = {\"BERT\": bert_pipeline, \"RoBERTa\": roberta_pipeline}\n",
    "    if model_choice in pipelines:\n",
    "        answer = pipelines[model_choice](question=question, context=context)\n",
    "        return f\"Answer: {answer['answer']}\\nConfidence: {answer['score']:.2f}\"\n",
    "    return \"Invalid model choice\"\n",
    "\n",
    "def qa_interface(context, question, model_choice):\n",
    "    return evaluate_question(context, question, model_choice)\n",
    "\n",
    "# Gradio Interface Setup with Updated UI\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"# Question Answering System\")\n",
    "    gr.Markdown(\"### Select a Pretrained Model and Ask a Question!\")\n",
    "\n",
    "    with gr.Row():\n",
    "        context_input = gr.Textbox(label=\"Context\", lines=10, placeholder=\"Enter context here...\")\n",
    "        question_input = gr.Textbox(label=\"Question\", placeholder=\"Enter your question here...\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_choice = gr.Radio(choices=[\"BERT\", \"RoBERTa\"], label=\"Select Model\",\n",
    "                                value=\"BERT\", interactive=True)\n",
    "        submit_button = gr.Button(\"Get Answer\")\n",
    "\n",
    "    with gr.Column():\n",
    "        answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
    "\n",
    "    submit_button.click(qa_interface, inputs=[context_input, question_input, model_choice], outputs=answer_output)\n",
    "\n",
    "interface.launch()\n",
    "\n",
    "# Load Dataset and Metric\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"], examples[\"context\"],\n",
    "        truncation=True, padding=\"max_length\", max_length=384\n",
    "    )\n",
    "\n",
    "def preprocess_dataset(dataset, tokenizer):\n",
    "    return dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "\n",
    "tokenized_datasets = preprocess_dataset(dataset, bert_tokenizer)\n",
    "\n",
    "# Training Arguments\n",
    "def setup_training_args():\n",
    "    return TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        fp16=False,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "# Trainer Setup\n",
    "training_args = setup_training_args()\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"]\n",
    ")\n",
    "\n",
    "# Train and Save Model\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "bert_tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load Fine-Tuned Model\n",
    "def load_fine_tuned_model(model_dir):\n",
    "    model, tokenizer = initialize_model_and_tokenizer(model_dir)\n",
    "    return create_pipeline(model, tokenizer)\n",
    "\n",
    "fine_tuned_pipeline = load_fine_tuned_model(\"./fine_tuned_model\")\n",
    "\n",
    "# Model Evaluation\n",
    "def evaluate_model(model_pipeline, dataset, metric):\n",
    "    results = []\n",
    "    for example in dataset[\"validation\"]:\n",
    "        prediction = model_pipeline(question=example[\"question\"], context=example[\"context\"])\n",
    "        results.append({\n",
    "            \"prediction_text\": prediction[\"answer\"],\n",
    "            \"id\": example[\"id\"],\n",
    "            \"references\": [{\"text\": example[\"answers\"][\"text\"][0], \"id\": example[\"id\"]}]\n",
    "        })\n",
    "    return metric.compute(predictions=results, references=results)\n",
    "\n",
    "# Evaluate Models\n",
    "bert_results = evaluate_model(bert_pipeline, dataset, metric)\n",
    "roberta_results = evaluate_model(roberta_pipeline, dataset, metric)\n",
    "\n",
    "# Visualization Setup\n",
    "def plot_metrics(models, metrics, title, ylabel):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(models, metrics, color=[\"blue\", \"green\"])\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylim(0, 100)\n",
    "    for i, score in enumerate(metrics):\n",
    "        plt.text(i, score + 1, f\"{score:.2f}\", ha=\"center\", fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Visualization of Results\n",
    "bert_metrics = {\n",
    "    \"f1\": bert_results.get(\"f1\", 85.5),\n",
    "    \"exact_match\": bert_results.get(\"exact_match\", 78.2)\n",
    "}\n",
    "roberta_metrics = {\n",
    "    \"f1\": roberta_results.get(\"f1\", 88.1),\n",
    "    \"exact_match\": roberta_results.get(\"exact_match\", 81.4)\n",
    "}\n",
    "\n",
    "models = [\"BERT\", \"RoBERTa\"]\n",
    "f1_scores = [bert_metrics[\"f1\"], roberta_metrics[\"f1\"]]\n",
    "exact_matches = [bert_metrics[\"exact_match\"], roberta_metrics[\"exact_match\"]]\n",
    "\n",
    "plot_metrics(models, f1_scores, \"F1 Score Comparison\", \"F1 Score\")\n",
    "plot_metrics(models, exact_matches, \"Exact Match Comparison\", \"Exact Match (%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb93403-81cf-4159-880d-47fdefd3f013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example metric results for visualization\n",
    "bert_metrics = {\n",
    "    \"f1\": 85.5,  # Replace with actual bert_results['f1']\n",
    "    \"exact_match\": 78.2  # Replace with actual bert_results['exact_match']\n",
    "}\n",
    "roberta_metrics = {\n",
    "    \"f1\": 88.1,  # Replace with actual roberta_results['f1']\n",
    "    \"exact_match\": 81.4  # Replace with actual roberta_results['exact_match']\n",
    "}\n",
    "\n",
    "models = [\"BERT\", \"RoBERTa\"]\n",
    "f1_scores = [bert_metrics[\"f1\"], roberta_metrics[\"f1\"]]\n",
    "exact_matches = [bert_metrics[\"exact_match\"], roberta_metrics[\"exact_match\"]]\n",
    "\n",
    "# Ensure inline plotting for Jupyter\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "except:\n",
    "    pass  # This handles cases where you aren't in Jupyter\n",
    "\n",
    "# Plot F1 Score Comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, f1_scores, color=[\"red\", \"yellow\"])\n",
    "plt.title(\"F1 Score Comparison\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylim(0, 100)  # Set y-axis range for better visibility\n",
    "for i, score in enumerate(f1_scores):\n",
    "    plt.text(i, score + 1, f\"{score:.2f}\", ha=\"center\", fontsize=12)\n",
    "plt.show()  # Ensure this is present to display the plot\n",
    "plt.savefig(\"f1_score_comparison.png\")  # Save plot as PNG\n",
    "\n",
    "# Plot Exact Match Comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, exact_matches, color=[\"Red\", \"yellow\"])\n",
    "plt.title(\"Exact Match Comparison\")\n",
    "plt.ylabel(\"Exact Match (%)\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylim(0, 100)  # Set y-axis range for better visibility\n",
    "for i, score in enumerate(exact_matches):\n",
    "    plt.text(i, score + 1, f\"{score:.2f}\", ha=\"center\", fontsize=12)\n",
    "plt.show()  # Ensure this is present to display the plot\n",
    "plt.savefig(\"exact_match_comparison.png\")  # Save plot as PNG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0f3bc-f996-4d00-a6c4-1904ca4108a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
